{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet Classifier- NLP BERT and GloVe.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDLPNc6FxuFy",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing with BERT and GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E07pQUPzyBu8",
        "colab_type": "text"
      },
      "source": [
        "The Real or Not:  NLP with Disaster Tweets Data is a collection of tweets in english taken from twitter. The data comes with the initial text, a key word column, and a location column.\n",
        "\n",
        "The Objective of this notebook is to predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0. The two types of RNN models I chose for this paper are BERT and GloVe.\n",
        "\n",
        "\n",
        "BERT stands for Bidirectional Encoder Representations from Transformers. BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).\n",
        "\n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n",
        "\n",
        "For my BERT model my hyperparameters consisted of 3 epochs and a batch size of 16. \n",
        "\n",
        "For my GloVe model my hyperparameters consisted of 400 iterations at a learning rate of 0.01.\n",
        "\n",
        "As for the results of the models on Kaggle, the BERT Model got a score of 0.83333, while the GloVe Model got a 0.66871.\n",
        "\n",
        "If management is thinking about using a language model to classify written customer reviews and call and complaint logs, and the most critical customer messages can be identified, then customer support personnel could be assigned to contact those customers. The key transformation which needs to be made is to calibrate the models to look for sentiment as opposed to simply looking for whether or not a catastrophe was being referenced. The model could theoretically be used to respond to customer emails. \n",
        "\n",
        "If management wanted to use this to respond to calls instead of emails, then a method to encode the customer's speech into text would be necessay, since the models take text as an input and then later encodes it.\n",
        "\n",
        "This notebook was inspired by two notebooks:\n",
        "\n",
        "https://www.kaggle.com/mashiat/nlp-rnn\n",
        "\n",
        "https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxIjWjYaxpj_",
        "colab_type": "text"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtT7e3SSf0m-",
        "colab_type": "code",
        "outputId": "42215ac9-de5f-4bbb-ae42-f6c6e656ea9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "tf.test.gpu_device_name() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3R3yAAxf2Cu",
        "colab_type": "code",
        "outputId": "612ec112-5394-4c91-c3b1-30000bb203c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/mntDrive') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /mntDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06vlB395f6Z7",
        "colab_type": "code",
        "outputId": "8d80596a-f048-47f3-9bdc-4157a6224343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "! ls \"/mntDrive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 403\t\t\t\t     'submission_1 assignment 5.csv'\n",
            "'Colab Notebooks'\t\t     'submission_2 assignment 5.csv'\n",
            " dogs-vs-cats-redux-kernels-edition   test.csv\n",
            " nlp-getting-started\t\t      train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh7jZB18eBEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use the official tokenization script created by the Google team\n",
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWNpzoaceZfD",
        "colab_type": "code",
        "outputId": "18a7d120-eb0b-4f0e-e756-aee0e03c8dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip3 install tensorflow_text>=2.0.0rc0\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 4.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 4.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BB2ng4eEAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import tokenization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 150)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVrWjmVdfK2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"/mntDrive/My Drive/nlp-getting-started/train.csv\")\n",
        "test = pd.read_csv(\"/mntDrive/My Drive/nlp-getting-started/test.csv\")\n",
        "submission=pd.read_csv(\"/mntDrive/My Drive/nlp-getting-started/sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgoq5n3rnvps",
        "colab_type": "text"
      },
      "source": [
        "# BERT Model Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIQ1kp6IeKLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_encode(texts, tokenizer, max_len=512):    \n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_60ZIdoQe1Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(clf_output)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEqqm8LWe9Gx",
        "colab_type": "code",
        "outputId": "08baab45-f0b1-4493-d2d5-c3d4f0f21ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21.6 s, sys: 3.91 s, total: 25.5 s\n",
            "Wall time: 26.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFU2oQoMfTFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2APF80_fbgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
        "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
        "train_labels = train.target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beBwImXFoQil",
        "colab_type": "text"
      },
      "source": [
        "# BERT Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhkHbNY3ffJ0",
        "colab_type": "code",
        "outputId": "5fc76e56-056a-4615-d274-e498f2cb2df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "model = build_model(bert_layer, max_len=160)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
            "==================================================================================================\n",
            "Total params: 335,142,914\n",
            "Trainable params: 335,142,913\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atFo4YY9ff-Q",
        "colab_type": "code",
        "outputId": "cda508ef-18da-4c90-8c95-371be43d2b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "checkpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "train_history = model.fit(\n",
        "    train_input, train_labels,\n",
        "    validation_split=0.2,\n",
        "    epochs=3,\n",
        "    callbacks=[checkpoint],\n",
        "    batch_size=16\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "381/381 [==============================] - 448s 1s/step - loss: 0.2648 - accuracy: 0.8933 - val_loss: 0.3967 - val_accuracy: 0.8418\n",
            "Epoch 2/3\n",
            "381/381 [==============================] - 373s 980ms/step - loss: 0.1438 - accuracy: 0.9496 - val_loss: 0.5233 - val_accuracy: 0.8253\n",
            "Epoch 3/3\n",
            "381/381 [==============================] - 373s 980ms/step - loss: 0.0853 - accuracy: 0.9713 - val_loss: 0.6126 - val_accuracy: 0.8162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfI-L2SjoZwc",
        "colab_type": "text"
      },
      "source": [
        "#BERT Model on Kaggle Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPS-JeUHfmnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('model.h5')\n",
        "test_pred = model.predict(test_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn-krITyfqFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission['target'] = test_pred.round().astype(int)\n",
        "submission.to_csv('Assignment 8 BERT submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG_85wR8oxV4",
        "colab_type": "text"
      },
      "source": [
        "# GloVe Model Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrnBMmFJpBRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading from the file to turn the words to word embedding vector\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdaC6HjYo6EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading from the file to learn the word embedding into the list word_to_vec_map\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/mntDrive/My Drive/nlp-getting-started/glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7izvGcQdpEN-",
        "colab_type": "code",
        "outputId": "d796b54e-02b7-4029-f41a-579f8b68170e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "print(train[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       Our Deeds are the Reason of this #earthquake M...\n",
            "1                  Forest fire near La Ronge Sask. Canada\n",
            "2       All residents asked to 'shelter in place' are ...\n",
            "3       13,000 people receive #wildfires evacuation or...\n",
            "4       Just got sent this photo from Ruby #Alaska as ...\n",
            "                              ...                        \n",
            "7608    Two giant cranes holding a bridge collapse int...\n",
            "7609    @aria_ahrary @TheTawniest The out of control w...\n",
            "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
            "7611    Police investigating after an e-bike collided ...\n",
            "7612    The Latest: More Homes Razed by Northern Calif...\n",
            "Name: text, Length: 7613, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U94tM1LepHT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(text):\n",
        "    regex = re.compile('([^\\s\\w]|_)+')\n",
        "    sentence = regex.sub('', text).lower()\n",
        "    sentence = sentence.split(\" \")\n",
        "    \n",
        "    for word in list(sentence):\n",
        "        if word not in word_to_vec_map:\n",
        "            sentence.remove(word)  \n",
        "            \n",
        "    sentence = \" \".join(sentence)\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KybMNsgtpJm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (train.shape[0]):\n",
        "    train.at[i,'text']=clean(train.loc[i,'text'])\n",
        "    \n",
        "for i in range(test.shape[0]):\n",
        "    test.at[i,'text']=clean(test.loc[i,'text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcsS4WzApMtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#determining the max length of a text in training set\n",
        "maxLen = len(max(train[\"text\"], key=len).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gl-N3QapO1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trying the length of the text of id=1\n",
        "length=len(str(train[train['id']==1][\"text\"]).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki1oA7y3pQ7Q",
        "colab_type": "code",
        "outputId": "9f38808b-271e-4e0d-e6db-ad7e1e5f4464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>our deeds are the reason of this earthquake ma...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>all residents asked to shelter in place are be...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>just got sent this photo from ruby alaska as s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  target\n",
              "0   1     NaN      NaN  our deeds are the reason of this earthquake ma...       1\n",
              "1   4     NaN      NaN              forest fire near la ronge sask canada       1\n",
              "2   5     NaN      NaN  all residents asked to shelter in place are be...       1\n",
              "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...       1\n",
              "4   7     NaN      NaN  just got sent this photo from ruby alaska as s...       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pAP_9r9pS4g",
        "colab_type": "code",
        "outputId": "89c35425-1711-45bf-e757-b634e78fcb2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#One hot encoding of the target to 2 dimensional vector\n",
        "Y_oh_train = tf.one_hot(train[\"target\"],2,dtype='int32')\n",
        "\n",
        "Y_oh_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHGpUdyapWWa",
        "colab_type": "code",
        "outputId": "4a8da5e9-2da2-4cb8-be17-44d90ad1faf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train[\"text\"].values[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'forest fire near la ronge sask canada'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9NjQfocpcIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
        "    and averages its value into a single vector encoding the meaning of the sentence.\n",
        "    \n",
        "    Arguments:\n",
        "    sentence -- string, one training example from X\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    \n",
        "    Returns:\n",
        "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Step 1: Split sentence into list of lower case words (≈ 1 line)\n",
        "    #sentence=str(sentence.translate(str.maketrans('', '', string.punctuation)))\n",
        "    words = (sentence.lower()).split()\n",
        "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
        "    avg = np.zeros((50,))\n",
        "    \n",
        "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
        "    #using try except disregard some words which doesn't exist in the glove Vector such as 'dtype'\n",
        "    total = 0\n",
        "    for w in words:\n",
        "        total += word_to_vec_map[w]\n",
        "    if len(words):\n",
        "        avg = total/len(words)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkjvLcMbpfQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkGm8K4ophxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, Y, W, b, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data containing sentences, numpy array of shape (m, None)\n",
        "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
        "    \n",
        "    Returns:\n",
        "    pred -- numpy array of shape (m, 1) with your predictions\n",
        "    \"\"\"\n",
        "    m = Y.shape[0]\n",
        "    pred = np.zeros((m, 1))\n",
        "    Y_oh=tf.one_hot(Y,n_y,dtype='int32')\n",
        "    Y=np.zeros((m,1))\n",
        "    \n",
        "    \n",
        "    for j in range(m):                       # Loop over training examples\n",
        "        \n",
        "        # Split jth test example (sentence) into list of lower case words\n",
        "        if Y_oh[j][0]==1:\n",
        "            Y[j]=0\n",
        "        else:\n",
        "            Y[j]=1\n",
        "        words = X[j].lower().split()\n",
        "        \n",
        "        avg = np.zeros((50,))\n",
        "    \n",
        "        total = 0\n",
        "        for w in words:\n",
        "            total += word_to_vec_map[w]\n",
        "        if len(words):\n",
        "            avg = total/len(words)\n",
        "        \n",
        "\n",
        "        # Forward propagation\n",
        "        Z = np.dot(W, avg) + b\n",
        "        A = softmax(Z)\n",
        "        pred[j] = np.argmax(A)\n",
        "        \n",
        "    print(\"Accuracy: \"  + str(np.mean((pred[:] == np.reshape(Y,(Y.shape[0],1)[:])))))\n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ_030YepmLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
        "    \"\"\"\n",
        "    Model to train word vector representations in numpy.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
        "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
        "    num_iterations -- number of iterations\n",
        "    \n",
        "    Returns:\n",
        "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
        "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
        "    b -- bias of the softmax layer, of shape (n_y,)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Define number of training examples\n",
        "    m = Y.shape[0]                          # number of training examples\n",
        "    n_y = 2                                # number of classes  \n",
        "    n_h = 50                                # dimensions of the GloVe vectors \n",
        "    \n",
        "    # Initialize parameters using Xavier initialization\n",
        "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b = np.zeros((n_y,1))\n",
        "    pred=np.zeros((m,1))\n",
        "    \n",
        "    # Convert Y to Y_onehot with n_y classes\n",
        "    Y_oh=tf.one_hot(Y,n_y,dtype='int32')\n",
        "    \n",
        "    # Optimization loop\n",
        "    for t in range(num_iterations):# Loop over the number of iterations\n",
        "        print(\"Number of iterations\",t)\n",
        "        for i in range(m):          # Loop over the training examples\n",
        "            \n",
        "            ### START CODE HERE ### (≈ 4 lines of code)\n",
        "            # Average the word vectors of the words from the i'th training example\n",
        "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
        "            avg=np.reshape(avg,(n_h,1))\n",
        "\n",
        "            # Forward propagate the avg through the softmax layer\n",
        "            z = np.dot(W,avg)+b\n",
        "            a = softmax(z)\n",
        "\n",
        "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
        "            cost =-np.dot(np.transpose(Y_oh[i]),np.log(a))\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            # Compute gradients \n",
        "            Y_oh_try=np.reshape(Y_oh[i],(n_y,1))\n",
        "            dz = a - Y_oh_try\n",
        "            dz=np.reshape(dz,(n_y,1))\n",
        "            avg=np.reshape(avg,(1, n_h))\n",
        "            dW = np.dot(dz,avg)\n",
        "            db = dz\n",
        "\n",
        "            # Update parameters with Stochastic Gradient Descent\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "        \n",
        "        if t % 100 == 0:\n",
        "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
        "            print(Y.shape)\n",
        "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
        "        \n",
        "    return pred, W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emyT-KxJpwsn",
        "colab_type": "text"
      },
      "source": [
        "# GloVe Model Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9foocBnqAZK",
        "colab_type": "code",
        "outputId": "bf7e22d7-97a7-4da7-f80d-f5052607942a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "print(train[\"text\"].shape)\n",
        "print(Y_oh_train[0].shape)\n",
        "X=train[\"text\"]\n",
        "n_y=2\n",
        "n_h=50\n",
        "W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "b = np.zeros((n_y,1))\n",
        "avg = sentence_to_avg(X[0], word_to_vec_map)\n",
        "avg=np.reshape(avg,(n_h,1))\n",
        "# Forward propagate the avg through the softmax layer\n",
        "z = np.dot(W,avg)+b\n",
        "a = softmax(z)\n",
        "print(\"shape of b\",b.shape)\n",
        "print(\"shape of W\",W.shape)\n",
        "print(\"shape of avg\",avg.shape)\n",
        "print(\"z shape\",z.shape)\n",
        "print()\n",
        "cost =-np.dot(np.transpose(Y_oh_train[0]),np.log(a))\n",
        "dz = a - Y_oh_train[0]\n",
        "print(\"a shape\",a.shape)\n",
        "print(\"y_oh shape\",train[\"target\"].shape)\n",
        "print(\"X_shape\",train[\"text\"].shape)\n",
        "print(\"shape of dz\",dz.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613,)\n",
            "(2,)\n",
            "shape of b (2, 1)\n",
            "shape of W (2, 50)\n",
            "shape of avg (50, 1)\n",
            "z shape (2, 1)\n",
            "\n",
            "a shape (2, 1)\n",
            "y_oh shape (7613,)\n",
            "X_shape (7613,)\n",
            "shape of dz (2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh8hYqTNqD1A",
        "colab_type": "code",
        "outputId": "3785ad49-d56c-4976-e09b-5bcfb0f508b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7280
        }
      },
      "source": [
        "pred, W, b = model(train[\"text\"], train[\"target\"], word_to_vec_map)\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of iterations 0\n",
            "Epoch: 0 --- cost = [0.05533201]\n",
            "(7613,)\n",
            "Accuracy: 0.7166688559043741\n",
            "Number of iterations 1\n",
            "Number of iterations 2\n",
            "Number of iterations 3\n",
            "Number of iterations 4\n",
            "Number of iterations 5\n",
            "Number of iterations 6\n",
            "Number of iterations 7\n",
            "Number of iterations 8\n",
            "Number of iterations 9\n",
            "Number of iterations 10\n",
            "Number of iterations 11\n",
            "Number of iterations 12\n",
            "Number of iterations 13\n",
            "Number of iterations 14\n",
            "Number of iterations 15\n",
            "Number of iterations 16\n",
            "Number of iterations 17\n",
            "Number of iterations 18\n",
            "Number of iterations 19\n",
            "Number of iterations 20\n",
            "Number of iterations 21\n",
            "Number of iterations 22\n",
            "Number of iterations 23\n",
            "Number of iterations 24\n",
            "Number of iterations 25\n",
            "Number of iterations 26\n",
            "Number of iterations 27\n",
            "Number of iterations 28\n",
            "Number of iterations 29\n",
            "Number of iterations 30\n",
            "Number of iterations 31\n",
            "Number of iterations 32\n",
            "Number of iterations 33\n",
            "Number of iterations 34\n",
            "Number of iterations 35\n",
            "Number of iterations 36\n",
            "Number of iterations 37\n",
            "Number of iterations 38\n",
            "Number of iterations 39\n",
            "Number of iterations 40\n",
            "Number of iterations 41\n",
            "Number of iterations 42\n",
            "Number of iterations 43\n",
            "Number of iterations 44\n",
            "Number of iterations 45\n",
            "Number of iterations 46\n",
            "Number of iterations 47\n",
            "Number of iterations 48\n",
            "Number of iterations 49\n",
            "Number of iterations 50\n",
            "Number of iterations 51\n",
            "Number of iterations 52\n",
            "Number of iterations 53\n",
            "Number of iterations 54\n",
            "Number of iterations 55\n",
            "Number of iterations 56\n",
            "Number of iterations 57\n",
            "Number of iterations 58\n",
            "Number of iterations 59\n",
            "Number of iterations 60\n",
            "Number of iterations 61\n",
            "Number of iterations 62\n",
            "Number of iterations 63\n",
            "Number of iterations 64\n",
            "Number of iterations 65\n",
            "Number of iterations 66\n",
            "Number of iterations 67\n",
            "Number of iterations 68\n",
            "Number of iterations 69\n",
            "Number of iterations 70\n",
            "Number of iterations 71\n",
            "Number of iterations 72\n",
            "Number of iterations 73\n",
            "Number of iterations 74\n",
            "Number of iterations 75\n",
            "Number of iterations 76\n",
            "Number of iterations 77\n",
            "Number of iterations 78\n",
            "Number of iterations 79\n",
            "Number of iterations 80\n",
            "Number of iterations 81\n",
            "Number of iterations 82\n",
            "Number of iterations 83\n",
            "Number of iterations 84\n",
            "Number of iterations 85\n",
            "Number of iterations 86\n",
            "Number of iterations 87\n",
            "Number of iterations 88\n",
            "Number of iterations 89\n",
            "Number of iterations 90\n",
            "Number of iterations 91\n",
            "Number of iterations 92\n",
            "Number of iterations 93\n",
            "Number of iterations 94\n",
            "Number of iterations 95\n",
            "Number of iterations 96\n",
            "Number of iterations 97\n",
            "Number of iterations 98\n",
            "Number of iterations 99\n",
            "Number of iterations 100\n",
            "Epoch: 100 --- cost = [0.05004785]\n",
            "(7613,)\n",
            "Accuracy: 0.660974648627348\n",
            "Number of iterations 101\n",
            "Number of iterations 102\n",
            "Number of iterations 103\n",
            "Number of iterations 104\n",
            "Number of iterations 105\n",
            "Number of iterations 106\n",
            "Number of iterations 107\n",
            "Number of iterations 108\n",
            "Number of iterations 109\n",
            "Number of iterations 110\n",
            "Number of iterations 111\n",
            "Number of iterations 112\n",
            "Number of iterations 113\n",
            "Number of iterations 114\n",
            "Number of iterations 115\n",
            "Number of iterations 116\n",
            "Number of iterations 117\n",
            "Number of iterations 118\n",
            "Number of iterations 119\n",
            "Number of iterations 120\n",
            "Number of iterations 121\n",
            "Number of iterations 122\n",
            "Number of iterations 123\n",
            "Number of iterations 124\n",
            "Number of iterations 125\n",
            "Number of iterations 126\n",
            "Number of iterations 127\n",
            "Number of iterations 128\n",
            "Number of iterations 129\n",
            "Number of iterations 130\n",
            "Number of iterations 131\n",
            "Number of iterations 132\n",
            "Number of iterations 133\n",
            "Number of iterations 134\n",
            "Number of iterations 135\n",
            "Number of iterations 136\n",
            "Number of iterations 137\n",
            "Number of iterations 138\n",
            "Number of iterations 139\n",
            "Number of iterations 140\n",
            "Number of iterations 141\n",
            "Number of iterations 142\n",
            "Number of iterations 143\n",
            "Number of iterations 144\n",
            "Number of iterations 145\n",
            "Number of iterations 146\n",
            "Number of iterations 147\n",
            "Number of iterations 148\n",
            "Number of iterations 149\n",
            "Number of iterations 150\n",
            "Number of iterations 151\n",
            "Number of iterations 152\n",
            "Number of iterations 153\n",
            "Number of iterations 154\n",
            "Number of iterations 155\n",
            "Number of iterations 156\n",
            "Number of iterations 157\n",
            "Number of iterations 158\n",
            "Number of iterations 159\n",
            "Number of iterations 160\n",
            "Number of iterations 161\n",
            "Number of iterations 162\n",
            "Number of iterations 163\n",
            "Number of iterations 164\n",
            "Number of iterations 165\n",
            "Number of iterations 166\n",
            "Number of iterations 167\n",
            "Number of iterations 168\n",
            "Number of iterations 169\n",
            "Number of iterations 170\n",
            "Number of iterations 171\n",
            "Number of iterations 172\n",
            "Number of iterations 173\n",
            "Number of iterations 174\n",
            "Number of iterations 175\n",
            "Number of iterations 176\n",
            "Number of iterations 177\n",
            "Number of iterations 178\n",
            "Number of iterations 179\n",
            "Number of iterations 180\n",
            "Number of iterations 181\n",
            "Number of iterations 182\n",
            "Number of iterations 183\n",
            "Number of iterations 184\n",
            "Number of iterations 185\n",
            "Number of iterations 186\n",
            "Number of iterations 187\n",
            "Number of iterations 188\n",
            "Number of iterations 189\n",
            "Number of iterations 190\n",
            "Number of iterations 191\n",
            "Number of iterations 192\n",
            "Number of iterations 193\n",
            "Number of iterations 194\n",
            "Number of iterations 195\n",
            "Number of iterations 196\n",
            "Number of iterations 197\n",
            "Number of iterations 198\n",
            "Number of iterations 199\n",
            "Number of iterations 200\n",
            "Epoch: 200 --- cost = [0.05022393]\n",
            "(7613,)\n",
            "Accuracy: 0.6779193484828583\n",
            "Number of iterations 201\n",
            "Number of iterations 202\n",
            "Number of iterations 203\n",
            "Number of iterations 204\n",
            "Number of iterations 205\n",
            "Number of iterations 206\n",
            "Number of iterations 207\n",
            "Number of iterations 208\n",
            "Number of iterations 209\n",
            "Number of iterations 210\n",
            "Number of iterations 211\n",
            "Number of iterations 212\n",
            "Number of iterations 213\n",
            "Number of iterations 214\n",
            "Number of iterations 215\n",
            "Number of iterations 216\n",
            "Number of iterations 217\n",
            "Number of iterations 218\n",
            "Number of iterations 219\n",
            "Number of iterations 220\n",
            "Number of iterations 221\n",
            "Number of iterations 222\n",
            "Number of iterations 223\n",
            "Number of iterations 224\n",
            "Number of iterations 225\n",
            "Number of iterations 226\n",
            "Number of iterations 227\n",
            "Number of iterations 228\n",
            "Number of iterations 229\n",
            "Number of iterations 230\n",
            "Number of iterations 231\n",
            "Number of iterations 232\n",
            "Number of iterations 233\n",
            "Number of iterations 234\n",
            "Number of iterations 235\n",
            "Number of iterations 236\n",
            "Number of iterations 237\n",
            "Number of iterations 238\n",
            "Number of iterations 239\n",
            "Number of iterations 240\n",
            "Number of iterations 241\n",
            "Number of iterations 242\n",
            "Number of iterations 243\n",
            "Number of iterations 244\n",
            "Number of iterations 245\n",
            "Number of iterations 246\n",
            "Number of iterations 247\n",
            "Number of iterations 248\n",
            "Number of iterations 249\n",
            "Number of iterations 250\n",
            "Number of iterations 251\n",
            "Number of iterations 252\n",
            "Number of iterations 253\n",
            "Number of iterations 254\n",
            "Number of iterations 255\n",
            "Number of iterations 256\n",
            "Number of iterations 257\n",
            "Number of iterations 258\n",
            "Number of iterations 259\n",
            "Number of iterations 260\n",
            "Number of iterations 261\n",
            "Number of iterations 262\n",
            "Number of iterations 263\n",
            "Number of iterations 264\n",
            "Number of iterations 265\n",
            "Number of iterations 266\n",
            "Number of iterations 267\n",
            "Number of iterations 268\n",
            "Number of iterations 269\n",
            "Number of iterations 270\n",
            "Number of iterations 271\n",
            "Number of iterations 272\n",
            "Number of iterations 273\n",
            "Number of iterations 274\n",
            "Number of iterations 275\n",
            "Number of iterations 276\n",
            "Number of iterations 277\n",
            "Number of iterations 278\n",
            "Number of iterations 279\n",
            "Number of iterations 280\n",
            "Number of iterations 281\n",
            "Number of iterations 282\n",
            "Number of iterations 283\n",
            "Number of iterations 284\n",
            "Number of iterations 285\n",
            "Number of iterations 286\n",
            "Number of iterations 287\n",
            "Number of iterations 288\n",
            "Number of iterations 289\n",
            "Number of iterations 290\n",
            "Number of iterations 291\n",
            "Number of iterations 292\n",
            "Number of iterations 293\n",
            "Number of iterations 294\n",
            "Number of iterations 295\n",
            "Number of iterations 296\n",
            "Number of iterations 297\n",
            "Number of iterations 298\n",
            "Number of iterations 299\n",
            "Number of iterations 300\n",
            "Epoch: 300 --- cost = [0.05018161]\n",
            "(7613,)\n",
            "Accuracy: 0.6886903980034152\n",
            "Number of iterations 301\n",
            "Number of iterations 302\n",
            "Number of iterations 303\n",
            "Number of iterations 304\n",
            "Number of iterations 305\n",
            "Number of iterations 306\n",
            "Number of iterations 307\n",
            "Number of iterations 308\n",
            "Number of iterations 309\n",
            "Number of iterations 310\n",
            "Number of iterations 311\n",
            "Number of iterations 312\n",
            "Number of iterations 313\n",
            "Number of iterations 314\n",
            "Number of iterations 315\n",
            "Number of iterations 316\n",
            "Number of iterations 317\n",
            "Number of iterations 318\n",
            "Number of iterations 319\n",
            "Number of iterations 320\n",
            "Number of iterations 321\n",
            "Number of iterations 322\n",
            "Number of iterations 323\n",
            "Number of iterations 324\n",
            "Number of iterations 325\n",
            "Number of iterations 326\n",
            "Number of iterations 327\n",
            "Number of iterations 328\n",
            "Number of iterations 329\n",
            "Number of iterations 330\n",
            "Number of iterations 331\n",
            "Number of iterations 332\n",
            "Number of iterations 333\n",
            "Number of iterations 334\n",
            "Number of iterations 335\n",
            "Number of iterations 336\n",
            "Number of iterations 337\n",
            "Number of iterations 338\n",
            "Number of iterations 339\n",
            "Number of iterations 340\n",
            "Number of iterations 341\n",
            "Number of iterations 342\n",
            "Number of iterations 343\n",
            "Number of iterations 344\n",
            "Number of iterations 345\n",
            "Number of iterations 346\n",
            "Number of iterations 347\n",
            "Number of iterations 348\n",
            "Number of iterations 349\n",
            "Number of iterations 350\n",
            "Number of iterations 351\n",
            "Number of iterations 352\n",
            "Number of iterations 353\n",
            "Number of iterations 354\n",
            "Number of iterations 355\n",
            "Number of iterations 356\n",
            "Number of iterations 357\n",
            "Number of iterations 358\n",
            "Number of iterations 359\n",
            "Number of iterations 360\n",
            "Number of iterations 361\n",
            "Number of iterations 362\n",
            "Number of iterations 363\n",
            "Number of iterations 364\n",
            "Number of iterations 365\n",
            "Number of iterations 366\n",
            "Number of iterations 367\n",
            "Number of iterations 368\n",
            "Number of iterations 369\n",
            "Number of iterations 370\n",
            "Number of iterations 371\n",
            "Number of iterations 372\n",
            "Number of iterations 373\n",
            "Number of iterations 374\n",
            "Number of iterations 375\n",
            "Number of iterations 376\n",
            "Number of iterations 377\n",
            "Number of iterations 378\n",
            "Number of iterations 379\n",
            "Number of iterations 380\n",
            "Number of iterations 381\n",
            "Number of iterations 382\n",
            "Number of iterations 383\n",
            "Number of iterations 384\n",
            "Number of iterations 385\n",
            "Number of iterations 386\n",
            "Number of iterations 387\n",
            "Number of iterations 388\n",
            "Number of iterations 389\n",
            "Number of iterations 390\n",
            "Number of iterations 391\n",
            "Number of iterations 392\n",
            "Number of iterations 393\n",
            "Number of iterations 394\n",
            "Number of iterations 395\n",
            "Number of iterations 396\n",
            "Number of iterations 397\n",
            "Number of iterations 398\n",
            "Number of iterations 399\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YwJR3GQqIqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre(X, W, b, word_to_vec_map):\n",
        "    \n",
        "    print(type(X))\n",
        "    m=X.shape[0]\n",
        "    \n",
        "    pred=np.zeros((m,1))\n",
        "    \n",
        "    \n",
        "    for j in range(m):                       # Loop over training examples\n",
        "        \n",
        "        # Split jth test example (sentence) into list of lower case words\n",
        "        words = X[j].lower().split()\n",
        "        \n",
        "        avg = np.zeros((50,))\n",
        "    \n",
        "        total = 0\n",
        "        for w in words:\n",
        "            total += word_to_vec_map[w]\n",
        "        if len(words):\n",
        "            avg = total/len(words)\n",
        "        \n",
        "\n",
        "        # Forward propagation\n",
        "        Z = np.dot(W, avg) + b\n",
        "        A = softmax(Z)\n",
        "        pred[j] = np.argmax(A)\n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMjyYwDoqO80",
        "colab_type": "text"
      },
      "source": [
        "# GloVe Model on Kaggle Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT-yT9-6qXS1",
        "colab_type": "code",
        "outputId": "ff05da83-3133-49fa-b272-9ab2d8849207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# %% [code]\n",
        "\n",
        "submission.head()\n",
        "# %% [code]\n",
        "submission[\"target\"]= pre(test[\"text\"], W, b, word_to_vec_map)\n",
        "submission[\"target\"]=submission[\"target\"].astype(int)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brB9zDGeqcht",
        "colab_type": "code",
        "outputId": "ed27aa73-e093-490e-b05a-0b7311038ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "submission.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target\n",
              "0   0       1\n",
              "1   2       1\n",
              "2   3       1\n",
              "3   9       1\n",
              "4  11       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T3_0W6JqdPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv(\"Assignment 8 GloVe submission.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}